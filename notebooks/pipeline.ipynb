{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "project_root = os.path.abspath(os.path.join(os.getcwd(), '..'))\n",
    "sys.path.append(project_root)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) Resize the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Folders: 100%|██████████| 3000/3000 [01:55<00:00, 26.06folder/s]\n"
     ]
    }
   ],
   "source": [
    "from src.utils.resize_features import process_files\n",
    "process_files(\"../data/da-tacos_benchmark_subset_hpcp\", \"../data/17K_data\", 17000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Validate the length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Unique HPCP frame lengths found: {17000}\n",
      "All files have the same HPCP frame length: 17000\n"
     ]
    }
   ],
   "source": [
    "from src.utils.check_length import check_hpcp_lengths\n",
    "\n",
    "check_hpcp_lengths(\"../data/17K_data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Make mappings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similar Pairs (Label 0): 9425\n",
      "Dissimilar Pairs (Label 1): 9994\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from src.utils.make_mappings import get_all_folders, create_song_pairs,remove_duplicate_pairs,check_pair_balance\n",
    "\n",
    "data = get_all_folders(\"../data/17K_data\")\n",
    "song_pairs = create_song_pairs(data, 10000)\n",
    "song_pairs_no_dup = remove_duplicate_pairs(song_pairs)\n",
    "check_pair_balance(song_pairs_no_dup)\n",
    "with open(\"song_pairs.json\", \"w\") as f:\n",
    "    json.dump(song_pairs, f, indent=4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Split data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset split: Train=160, Val=20, Test=20\n",
      "Processed data saved in '../data/data_model'\n"
     ]
    }
   ],
   "source": [
    "from src.utils.split_data import process_data\n",
    "\n",
    "process_data(\n",
    "    mapping_json=\"song_pairs.json\",\n",
    "    base_path=\"../data/17K_data\",\n",
    "    output_dir=\"../data/data_model\",\n",
    "    batch_size=100,\n",
    "    sample_ratio=0.01,\n",
    "    test_size=0.1,\n",
    "    val_size=0.1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import yaml\n",
    "from torch.utils.data import DataLoader\n",
    "from src.model import SiameseNetworkWithBatchNorm\n",
    "from src.dataset import HpcpDataset\n",
    "from src.train import train_siamese\n",
    "from src.evaluate import evaluate_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) Initialize the device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Initialize the dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = HpcpDataset(\"../data/data_model/train_features.h5\",\"../data/data_model/train_labels.h5\")\n",
    "val_dataset = HpcpDataset(\"../data/data_model/val_features.h5\", \"../data/data_model/val_labels.h5\")\n",
    "test_dataset = HpcpDataset(\"../data/data_model/test_features.h5\", \"../data/data_model/test_labels.h5\") \n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Initialize model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SiameseNetworkWithBatchNorm().to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Start Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on mps\n",
      "Training Started\n",
      "Epoch [1/3] | Train Loss: 9.7710 | Train Acc: 0.5125 | Val Loss: 0.2947 | Val Acc: 0.5000\n",
      "Model improved. Saving checkpoint.\n",
      "Epoch [2/3] | Train Loss: 6.4795 | Train Acc: 0.5125 | Val Loss: 0.9537 | Val Acc: 0.5000\n",
      "Early stopping patience: 1/5\n",
      "Epoch [3/3] | Train Loss: 1.6999 | Train Acc: 0.5125 | Val Loss: 1.0788 | Val Acc: 0.5000\n",
      "Early stopping patience: 2/5\n",
      "Training Complete.\n",
      "Starting Evaluation on Test Set.\n",
      "Metrics saved to log/2025-02-11/metrics.txt\n"
     ]
    }
   ],
   "source": [
    "train_siamese(\n",
    "    device=device,\n",
    "    model=model,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    test_loader=test_loader,\n",
    "    epochs=3,\n",
    "    lr=0.001,\n",
    "    patience=5\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note:\n",
    "Check in the notebooks directory:\n",
    "   - 2 log directories\n",
    "   - 1 models directory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Evaluate on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Metrics - Accuracy: 0.5500, Precision: 0.4706, Recall: 1.0000, F1: 0.6400, AUC: 0.6250\n"
     ]
    }
   ],
   "source": [
    "evaluate_metrics(device, model, test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The metrics of this run should not be taken in mind as they were created by demostrating the pipeline of the code"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
